# -*- coding: utf-8 -*-
"""seminar_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CllYhjDSj-SgM3XaCrerofVNM0-l9FI3
"""

from google.colab import files
uploaded = files.upload()


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import xgboost as xgb
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                           f1_score, confusion_matrix, roc_curve, auc)

def main():
    """Main function to replicate the research results."""
    # Step 1: Load and prepare data
    print("Step 1: Loading and preparing data...")
    df, X_train, X_test, y_train, y_test = load_and_prepare_data()

    # Step 2: Define models
    print("Step 2: Defining models...")
    models = define_models()

    # Step 3: Train and evaluate models
    print("Step 3: Training and evaluating models...")
    results, y_probs = train_and_evaluate(models, X_train, X_test, y_train, y_test)

    # Step 4: Plot ROC curves
    print("Step 4: Plotting ROC curves...")
    plot_roc_curves(y_test, y_probs)

    # Step 5: Compare with paper results
    print("Step 5: Comparing with paper results...")
    compare_with_paper(results)

def load_and_prepare_data(file_path='features(1).xlsx'):
    """Load dataset from XLSX and prepare it for modeling."""
    # Load data
    print(f"Loading data from {file_path}...")
    df = pd.read_excel(file_path)

    # Display data info
    print("\nDataset information:")
    print(f"Total records: {len(df)}")
    print(f"Columns: {', '.join(df.columns)}")

    # Convert labels to numeric
    label_encoder = LabelEncoder()
    df['LABEL_ENCODED'] = label_encoder.fit_transform(df['label'])

    # Show class distribution
    class_distribution = df['label'].value_counts()
    print("\nClass distribution:")
    print(f"Benign samples: {class_distribution.get('B', 0)}")
    print(f"Malicious samples: {class_distribution.get('M', 0)}")

    # Prepare features and target
   # X = df.drop("label", axis=1)
    X = df[['r', 'rw', 'rx', 'rwc', 'rwx', 'rwxc']]
    y = df['LABEL_ENCODED']

    # Split data with 80/20 ratio and stratified sampling
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42
    )

    print(f"Training set size: {len(X_train)}")
    print(f"Testing set size: {len(X_test)}")

    return df, X_train, X_test, y_train, y_test

def define_models():
    """Define the machine learning models with parameters matching the paper."""
    models = {

        'Decision Tree': DecisionTreeClassifier(
            criterion='gini',
            min_samples_leaf=5,
            random_state=42
        ),

        'Tree Ensemble': RandomForestClassifier(
            n_estimators=100,
            criterion='entropy',
            max_features='sqrt',
            random_state=42
        ),

        'Random Forest': RandomForestClassifier(
            n_estimators=100,
            criterion='gini',
            random_state=42
        ),

        'Gradient Boosted Tree': GradientBoostingClassifier(
            n_estimators=100,
            random_state=42
        ),

        'XGBoost': xgb.XGBClassifier(
            objective='binary:logistic',
            n_estimators=1000,
            random_state=42
        ),

        'Naive Bayes': GaussianNB(),

        'SVM': SVC(
            kernel='rbf',
            gamma=0.1,
            probability=True,
            random_state=42
        ),
        #tehila
        'Neural Network': MLPClassifier(
            hidden_layer_sizes=(15, 15),
            max_iter=10000,
            activation='relu',
            solver='adam',  # Using adam as a substitute for rprop if not available
            random_state=42
        )
    }
    return models

def train_and_evaluate(models, X_train, X_test, y_train, y_test):
    """Train and evaluate each model, reporting detailed metrics."""
    results = {}
    y_probs = {}

    # Prepare scaled data for Neural Network
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    for name, model in models.items():
        print(f"\nTraining and evaluating {name}...")

        # Use scaled data for Neural Network
        if name == 'Neural Network':
            model.fit(X_train_scaled, y_train)
            y_pred = model.predict(X_test_scaled)
            if hasattr(model, "predict_proba"):
                y_prob = model.predict_proba(X_test_scaled)[:, 1]
            else:
                y_prob = model.decision_function(X_test_scaled)
        else:
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            if hasattr(model, "predict_proba"):
                y_prob = model.predict_proba(X_test)[:, 1]
            else:
                y_prob = model.decision_function(X_test)

        y_probs[name] = y_prob

        # Calculate confusion matrix
        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f_measure = f1_score(y_test, y_pred)

        # Store results
        results[name] = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f_measure': f_measure,
            'TP': tp, 'FP': fp, 'TN': tn, 'FN': fn
        }

        # Print results in the format similar to the paper's tables
        print(f"Results for {name}:")
        print(f"{'Label':<8}{'TP':<8}{'FP':<8}{'TN':<8}{'FN':<8}{'Recall':<10}{'Precision':<12}{'F-measure':<12}{'Accuracy':<10}")

        # For benign class (B)
        b_recall = tn/(tn+fn) if (tn+fn) > 0 else 0
        b_precision = tn/(tn+fp) if (tn+fp) > 0 else 0
        b_fmeasure = 2*tn/(2*tn+fp+fn) if (2*tn+fp+fn) > 0 else 0
        print(f"{'B':<8}{tn:<8}{fn:<8}{fp:<8}{tp:<8}{b_recall:<10.4f}{b_precision:<12.4f}{b_fmeasure:<12.4f}")

        # For malicious class (M)
        m_recall = tp/(tp+fn) if (tp+fn) > 0 else 0
        m_precision = tp/(tp+fp) if (tp+fp) > 0 else 0
        m_fmeasure = 2*tp/(2*tp+fp+fn) if (2*tp+fp+fn) > 0 else 0
        print(f"{'M':<8}{tp:<8}{fp:<8}{tn:<8}{fn:<8}{m_recall:<10.4f}{m_precision:<12.4f}{m_fmeasure:<12.4f}")

        # Overall accuracy
        print(f"{'Overall':<8}{'':<8}{'':<8}{'':<8}{'':<8}{'':<10}{'':<12}{'':<12}{accuracy:<10.4f}")

    return results, y_probs

def plot_roc_curves(y_test, y_probs):
    """Plot ROC curves for all models similar to Figure 3 in the paper."""
    plt.figure(figsize=(10, 8))
    plt.title('ROC Curve')
    plt.plot([0, 1], [0, 1], 'k--')  # Random classifier line

    # Colors for the curves
    colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'orange']

    for (name, y_prob), color in zip(y_probs.items(), colors):
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, color=color, label=f'{name} (AUC = {roc_auc:.3f})')

    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc='lower right')
    plt.grid(True)
    plt.savefig('roc_curve.png', dpi=300)
    plt.show()

def compare_with_paper(results):
    """Create a comparison table between your results and the paper's results."""
    # Extract accuracies from results
    accuracies = {name: data['accuracy'] for name, data in results.items()}

    # Paper results as reported
    paper_accuracies = {
        'Decision Tree': 0.9362,
        'Tree Ensemble': 0.9574,
        'Random Forest': 0.9521,
        'Gradient Boosted Tree': 0.9468,
        'XGBoost': 0.9628,
        'Naive Bayes': 0.8138,
        'SVM': 0.8564,
        'Neural Network': 0.9362
    }

    # Create comparison DataFrame
    comparison = pd.DataFrame({
        'Your Implementation': pd.Series(accuracies),
        'Paper Results': pd.Series(paper_accuracies)
    })

    comparison['Difference'] = comparison['Your Implementation'] - comparison['Paper Results']

    print("\nAccuracy Comparison:")
    print(comparison)

    # Highlight the best performing model
    best_model = max(accuracies.items(), key=lambda x: x[1])
    print(f"\nBest model in your implementation: {best_model[0]} with accuracy {best_model[1]:.4f}")
    print(f"Best model in paper: XGBoost with accuracy 0.9628")

if __name__ == "__main__":
  main()

